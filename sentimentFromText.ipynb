{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Getting Data & Assigning Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('../smp_data/ME_NH_RI_reviews.csv', encoding='utf-8',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of any of the middle reviews, trying to balance positive and negative\n",
    "revPosNeg = review_df.ix[(review_df['overall']<=2.0) | (review_df['overall'] == 5.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Including only certain departments (these have over 800 profs for both men & women in both sentiments)\n",
    "revSet_df = revPosNeg[revPosNeg['department'].isin(['English','Mathematics','Biology','Psychology'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# return content words and their stems\n",
    "def content_stems(text, stopwords):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    content = [word for word in tokens if word.lower() not in stopwords]\n",
    "    stems = [stemmer.stem(t) for t in content]\n",
    "    return [content,stems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angelavierling-claassen/anaconda/envs/my_projects_env/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "revSet_df['words'] = revSet_df['cleanText'].apply(lambda x: content_stems(x,stopwords)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign sentiment positive or negative\n",
    "# need to do vectorized version of this\n",
    "def sentiment(row):\n",
    "    if row['overall'] == 5:\n",
    "        val = 1\n",
    "    elif row['overall'] <= 2.0:\n",
    "        val = 0\n",
    "    else:\n",
    "        val = -1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angelavierling-claassen/anaconda/envs/my_projects_env/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# assign sentiment to all the reviews\n",
    "revSet_df['sentiment'] = revSet_df.apply(sentiment,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "revSet_df = revSet_df[['profID','genderBest','department','sentiment','cleanText','words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sentiment Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-27-ece42a00ae76>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-ece42a00ae76>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    if 'worst' in myList:\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def simplePredict(myList):\n",
    "if 'worst' in myList:\n",
    "    return 0\n",
    "elif 'best' in myList:\n",
    "    return 1\n",
    "elif 'good' in myList:\n",
    "    return 1\n",
    "else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Sentiment from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only use reviews where text is not \"no comments\"\n",
    "revSet_df = revSet_df[revSet_df['cleanText']!=\"no comments \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pick out the train and the test sets\n",
    "train_set = revSet_df\n",
    "\n",
    "#pulling out the test set, can add in random_state=89 to be able to replicate results\n",
    "test_set = train_set.sample(frac=.2)\n",
    "train_set=train_set.drop(test_set.index)\n",
    "# training to the sentiment\n",
    "y_train = train_set['sentiment']\n",
    "y_test = test_set['sentiment']\n",
    "\n",
    "\n",
    "# Initialize TFIDF vectorizor object (no n-grams)\n",
    "vectorizer = TfidfVectorizer( max_features = 40000, sublinear_tf = True )\n",
    "\n",
    "# fit_transform() fits the model & and learns the vocab (features)\n",
    "# then it transforms the training data into feature vectors. \n",
    "# The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(train_set['cleanText'])\n",
    "\n",
    "# convert to array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vocab = feature names\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "# oob_score gives \"out of bag\" errors\n",
    "forest = RandomForestClassifier(n_estimators = 100,oob_score=True) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92722632875747979"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how does the train set do on the out of bag reviews?\n",
    "forest.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "# note need to update to latest sklearn which has this under sklearn.model_selection\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9023532 ,  0.90387154,  0.90737074,  0.8990099 ,  0.90649065])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cross validation is very costly\n",
    "# may just want to go with the oob score (note that it is very close to cv score)\n",
    "scores = cross_validation.cross_val_score(forest, train_data_features, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at result on train data\n",
    "trainResult = forest.predict(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8810,    17],\n",
       "       [    0, 13901]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix for train data\n",
    "confusion_matrix(train_set[\"sentiment\"], trainResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Output and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(test_set['cleanText'])\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"profID\" & sentiment prediction\n",
    "output = pd.DataFrame( data={'sentimentActual':test_set[\"sentiment\"], 'gender':test_set['genderBest'],'depart':test_set['department'], \"sentimentPredict\":result} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depart</th>\n",
       "      <th>gender</th>\n",
       "      <th>sentimentActual</th>\n",
       "      <th>sentimentPredict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64121</th>\n",
       "      <td>English</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105316</th>\n",
       "      <td>Psychology</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80222</th>\n",
       "      <td>Psychology</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136767</th>\n",
       "      <td>English</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131989</th>\n",
       "      <td>Mathematics</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             depart  gender  sentimentActual  sentimentPredict\n",
       "64121       English  female                0                 0\n",
       "105316   Psychology    male                1                 1\n",
       "80222    Psychology    male                0                 0\n",
       "136767      English  female                0                 0\n",
       "131989  Mathematics    male                0                 0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1904,  229],\n",
       "       [ 119, 3430]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_set[\"sentiment\"], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.89      0.92      2133\n",
      "          1       0.94      0.97      0.95      3549\n",
      "\n",
      "avg / total       0.94      0.94      0.94      5682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_set['sentiment'],result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Differences: Female Professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Female professors\n",
    "femaleSet_df = revPosNeg[(revPosNeg['department'].isin(['English','Mathematics','Biology','Psychology']))&(revPosNeg['genderBest']=='female')]\n",
    "femaleSet_df = revSet_df[['profID','genderBest','department','sentiment','cleanText','words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only use reviews where text is not \"no comments\"\n",
    "femaleSet_df = femaleSet_df[revSet_df['cleanText']!=\"no comments \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick out the train and the test sets\n",
    "train_set = femaleSet_df\n",
    "\n",
    "#pulling out the test set\n",
    "test_set = train_set.sample(frac=.2,random_state=2411)\n",
    "train_set=train_set.drop(test_set.index)\n",
    "# training to the sentiment\n",
    "y_train = train_set['sentiment']\n",
    "y_test = test_set['sentiment']\n",
    "\n",
    "\n",
    "# Initialize TFIDF vectorizor object (no n-grams)\n",
    "vectorizer = TfidfVectorizer( max_features = 40000, sublinear_tf = True )\n",
    "\n",
    "# fit_transform() fits the model & and learns the vocab (features)\n",
    "# then it transforms the training data into feature vectors. \n",
    "# The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(train_set['cleanText'])\n",
    "\n",
    "# convert to array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "# oob_score gives \"out of bag\" errors\n",
    "forest = RandomForestClassifier(n_estimators = 100,oob_score=True) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab = feature names\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92705033438929951"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how does the train set do on the out of bag reviews?\n",
    "forest.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(test_set['cleanText'])\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"profID\" & sentiment prediction\n",
    "output = pd.DataFrame( data={'sentimentActual':test_set[\"sentiment\"], 'gender':test_set['genderBest'],'depart':test_set['department'], \"sentimentPredict\":result} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "femImportances = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1950,  280],\n",
       "       [ 108, 3344]])"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_set[\"sentiment\"], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "femImp_df = pd.DataFrame({'vocab':vocab,'import':femImportances}).sort_values('import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imp_df = imp_df.reset_index()\n",
    "femImp_df = femImp_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impVocabBoth=pd.merge(imp_df[-100:],femImp_df[-100:],on='vocab',how='outer',suffixes=('_all','_fem'),indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index_all</th>\n",
       "      <th>import_all</th>\n",
       "      <th>vocab</th>\n",
       "      <th>index_fem</th>\n",
       "      <th>import_fem</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17865.0</td>\n",
       "      <td>5201.0</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>even</td>\n",
       "      <td>5181.0</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17866.0</td>\n",
       "      <td>2273.0</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>cares</td>\n",
       "      <td>2273.0</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17867.0</td>\n",
       "      <td>15155.0</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>students</td>\n",
       "      <td>15141.0</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17868.0</td>\n",
       "      <td>10810.0</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>only</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17869.0</td>\n",
       "      <td>6777.0</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>guy</td>\n",
       "      <td>6755.0</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17870.0</td>\n",
       "      <td>16262.0</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>too</td>\n",
       "      <td>16256.0</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17871.0</td>\n",
       "      <td>2716.0</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>clear</td>\n",
       "      <td>2721.0</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17872.0</td>\n",
       "      <td>8454.0</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>just</td>\n",
       "      <td>8401.0</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17873.0</td>\n",
       "      <td>9393.0</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>makes</td>\n",
       "      <td>9366.0</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17874.0</td>\n",
       "      <td>1773.0</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>book</td>\n",
       "      <td>1769.0</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17875.0</td>\n",
       "      <td>7244.0</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>him</td>\n",
       "      <td>7214.0</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17876.0</td>\n",
       "      <td>4043.0</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>did</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17877.0</td>\n",
       "      <td>8624.0</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>know</td>\n",
       "      <td>8577.0</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17878.0</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>be</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17879.0</td>\n",
       "      <td>17637.0</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>17619.0</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17880.0</td>\n",
       "      <td>17255.0</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>waste</td>\n",
       "      <td>17247.0</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17881.0</td>\n",
       "      <td>10433.0</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>nice</td>\n",
       "      <td>10397.0</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17882.0</td>\n",
       "      <td>7291.0</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>his</td>\n",
       "      <td>7259.0</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17883.0</td>\n",
       "      <td>17249.0</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>was</td>\n",
       "      <td>17241.0</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17884.0</td>\n",
       "      <td>15931.0</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>that</td>\n",
       "      <td>15938.0</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17885.0</td>\n",
       "      <td>16936.0</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>useless</td>\n",
       "      <td>16931.0</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17886.0</td>\n",
       "      <td>10881.0</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>or</td>\n",
       "      <td>10846.0</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17887.0</td>\n",
       "      <td>17527.0</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>will</td>\n",
       "      <td>17515.0</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17888.0</td>\n",
       "      <td>17578.0</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>with</td>\n",
       "      <td>17564.0</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17889.0</td>\n",
       "      <td>7703.0</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>impossible</td>\n",
       "      <td>7654.0</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17890.0</td>\n",
       "      <td>17531.0</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>willing</td>\n",
       "      <td>17519.0</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17891.0</td>\n",
       "      <td>7221.0</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>highly</td>\n",
       "      <td>7190.0</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17892.0</td>\n",
       "      <td>4044.0</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>didnt</td>\n",
       "      <td>4040.0</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>17893.0</td>\n",
       "      <td>9242.0</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>lot</td>\n",
       "      <td>9215.0</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17894.0</td>\n",
       "      <td>6809.0</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>had</td>\n",
       "      <td>6787.0</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>17938.0</td>\n",
       "      <td>16217.0</td>\n",
       "      <td>0.005646</td>\n",
       "      <td>to</td>\n",
       "      <td>16213.0</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>17939.0</td>\n",
       "      <td>12953.0</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>really</td>\n",
       "      <td>12942.0</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>17940.0</td>\n",
       "      <td>17885.0</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>you</td>\n",
       "      <td>17873.0</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>17941.0</td>\n",
       "      <td>10484.0</td>\n",
       "      <td>0.005844</td>\n",
       "      <td>no</td>\n",
       "      <td>10452.0</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>17942.0</td>\n",
       "      <td>6188.0</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>fun</td>\n",
       "      <td>6177.0</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>17943.0</td>\n",
       "      <td>15943.0</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>the</td>\n",
       "      <td>15949.0</td>\n",
       "      <td>0.005846</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>17944.0</td>\n",
       "      <td>15658.0</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>teach</td>\n",
       "      <td>15667.0</td>\n",
       "      <td>0.009459</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>17945.0</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>interesting</td>\n",
       "      <td>7987.0</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>17946.0</td>\n",
       "      <td>7100.0</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>helpful</td>\n",
       "      <td>7074.0</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>17947.0</td>\n",
       "      <td>8208.0</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>is</td>\n",
       "      <td>8147.0</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>17948.0</td>\n",
       "      <td>16625.0</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>unclear</td>\n",
       "      <td>16621.0</td>\n",
       "      <td>0.005713</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>17949.0</td>\n",
       "      <td>4686.0</td>\n",
       "      <td>0.007178</td>\n",
       "      <td>easy</td>\n",
       "      <td>4687.0</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>17950.0</td>\n",
       "      <td>15663.0</td>\n",
       "      <td>0.007429</td>\n",
       "      <td>teacher</td>\n",
       "      <td>15672.0</td>\n",
       "      <td>0.006905</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>17951.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>awful</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>17952.0</td>\n",
       "      <td>16043.0</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>this</td>\n",
       "      <td>16043.0</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>17953.0</td>\n",
       "      <td>15834.0</td>\n",
       "      <td>0.008204</td>\n",
       "      <td>terrible</td>\n",
       "      <td>15844.0</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>17954.0</td>\n",
       "      <td>1827.0</td>\n",
       "      <td>0.008227</td>\n",
       "      <td>boring</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>17955.0</td>\n",
       "      <td>4357.0</td>\n",
       "      <td>0.008529</td>\n",
       "      <td>doesnt</td>\n",
       "      <td>4355.0</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>17956.0</td>\n",
       "      <td>12055.0</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>professor</td>\n",
       "      <td>12047.0</td>\n",
       "      <td>0.007427</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>17957.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>avoid</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>17958.0</td>\n",
       "      <td>7393.0</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>horrible</td>\n",
       "      <td>7363.0</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>17959.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>and</td>\n",
       "      <td>601.0</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>17960.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>awesome</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>17961.0</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>0.014883</td>\n",
       "      <td>best</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>17962.0</td>\n",
       "      <td>6643.0</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>great</td>\n",
       "      <td>6628.0</td>\n",
       "      <td>0.027892</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>17963.0</td>\n",
       "      <td>17730.0</td>\n",
       "      <td>0.023580</td>\n",
       "      <td>worst</td>\n",
       "      <td>17718.0</td>\n",
       "      <td>0.020703</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>17964.0</td>\n",
       "      <td>10564.0</td>\n",
       "      <td>0.026432</td>\n",
       "      <td>not</td>\n",
       "      <td>10526.0</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>as</td>\n",
       "      <td>912.0</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dr</td>\n",
       "      <td>4457.0</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one</td>\n",
       "      <td>10747.0</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     level_0  index_all  import_all        vocab  index_fem  import_fem  \\\n",
       "0    17865.0     5201.0    0.002149         even     5181.0    0.002188   \n",
       "1    17866.0     2273.0    0.002150        cares     2273.0    0.002369   \n",
       "2    17867.0    15155.0    0.002157     students    15141.0    0.002213   \n",
       "3    17868.0    10810.0    0.002163         only        NaN         NaN   \n",
       "4    17869.0     6777.0    0.002203          guy     6755.0    0.002253   \n",
       "5    17870.0    16262.0    0.002224          too    16256.0    0.002107   \n",
       "6    17871.0     2716.0    0.002262        clear     2721.0    0.002156   \n",
       "7    17872.0     8454.0    0.002267         just     8401.0    0.002426   \n",
       "8    17873.0     9393.0    0.002288        makes     9366.0    0.002245   \n",
       "9    17874.0     1773.0    0.002304         book     1769.0    0.002266   \n",
       "10   17875.0     7244.0    0.002352          him     7214.0    0.002278   \n",
       "11   17876.0     4043.0    0.002379          did        NaN         NaN   \n",
       "12   17877.0     8624.0    0.002415         know     8577.0    0.002648   \n",
       "13   17878.0     1388.0    0.002418           be     1376.0    0.002363   \n",
       "14   17879.0    17637.0    0.002453    wonderful    17619.0    0.002432   \n",
       "15   17880.0    17255.0    0.002519        waste    17247.0    0.003319   \n",
       "16   17881.0    10433.0    0.002521         nice    10397.0    0.002173   \n",
       "17   17882.0     7291.0    0.002522          his     7259.0    0.002721   \n",
       "18   17883.0    17249.0    0.002527          was    17241.0    0.002580   \n",
       "19   17884.0    15931.0    0.002543         that    15938.0    0.002366   \n",
       "20   17885.0    16936.0    0.002551      useless    16931.0    0.002115   \n",
       "21   17886.0    10881.0    0.002552           or    10846.0    0.002441   \n",
       "22   17887.0    17527.0    0.002555         will    17515.0    0.002590   \n",
       "23   17888.0    17578.0    0.002591         with    17564.0    0.002589   \n",
       "24   17889.0     7703.0    0.002597   impossible     7654.0    0.002626   \n",
       "25   17890.0    17531.0    0.002599      willing    17519.0    0.003626   \n",
       "26   17891.0     7221.0    0.002629       highly     7190.0    0.002733   \n",
       "27   17892.0     4044.0    0.002670        didnt     4040.0    0.002374   \n",
       "28   17893.0     9242.0    0.002709          lot     9215.0    0.002386   \n",
       "29   17894.0     6809.0    0.002720          had     6787.0    0.002996   \n",
       "..       ...        ...         ...          ...        ...         ...   \n",
       "73   17938.0    16217.0    0.005646           to    16213.0    0.005653   \n",
       "74   17939.0    12953.0    0.005669       really    12942.0    0.004939   \n",
       "75   17940.0    17885.0    0.005681          you    17873.0    0.005161   \n",
       "76   17941.0    10484.0    0.005844           no    10452.0    0.006333   \n",
       "77   17942.0     6188.0    0.005884          fun     6177.0    0.006352   \n",
       "78   17943.0    15943.0    0.005970          the    15949.0    0.005846   \n",
       "79   17944.0    15658.0    0.006248        teach    15667.0    0.009459   \n",
       "80   17945.0     8044.0    0.006369  interesting     7987.0    0.006742   \n",
       "81   17946.0     7100.0    0.006634      helpful     7074.0    0.006818   \n",
       "82   17947.0     8208.0    0.006739           is     8147.0    0.007144   \n",
       "83   17948.0    16625.0    0.007143      unclear    16621.0    0.005713   \n",
       "84   17949.0     4686.0    0.007178         easy     4687.0    0.006025   \n",
       "85   17950.0    15663.0    0.007429      teacher    15672.0    0.006905   \n",
       "86   17951.0     1186.0    0.007955        awful     1167.0    0.007228   \n",
       "87   17952.0    16043.0    0.008135         this    16043.0    0.006219   \n",
       "88   17953.0    15834.0    0.008204     terrible    15844.0    0.006404   \n",
       "89   17954.0     1827.0    0.008227       boring     1820.0    0.006724   \n",
       "90   17955.0     4357.0    0.008529       doesnt     4355.0    0.009580   \n",
       "91   17956.0    12055.0    0.008581    professor    12047.0    0.007427   \n",
       "92   17957.0     1145.0    0.009292        avoid     1124.0    0.008455   \n",
       "93   17958.0     7393.0    0.009507     horrible     7363.0    0.008827   \n",
       "94   17959.0      607.0    0.010749          and      601.0    0.010363   \n",
       "95   17960.0     1170.0    0.011050      awesome     1150.0    0.010734   \n",
       "96   17961.0     1542.0    0.014883         best     1534.0    0.018739   \n",
       "97   17962.0     6643.0    0.023440        great     6628.0    0.027892   \n",
       "98   17963.0    17730.0    0.023580        worst    17718.0    0.020703   \n",
       "99   17964.0    10564.0    0.026432          not    10526.0    0.027931   \n",
       "100      NaN        NaN         NaN           as      912.0    0.002108   \n",
       "101      NaN        NaN         NaN           dr     4457.0    0.002196   \n",
       "102      NaN        NaN         NaN          one    10747.0    0.002517   \n",
       "\n",
       "         _merge  \n",
       "0          both  \n",
       "1          both  \n",
       "2          both  \n",
       "3     left_only  \n",
       "4          both  \n",
       "5          both  \n",
       "6          both  \n",
       "7          both  \n",
       "8          both  \n",
       "9          both  \n",
       "10         both  \n",
       "11    left_only  \n",
       "12         both  \n",
       "13         both  \n",
       "14         both  \n",
       "15         both  \n",
       "16         both  \n",
       "17         both  \n",
       "18         both  \n",
       "19         both  \n",
       "20         both  \n",
       "21         both  \n",
       "22         both  \n",
       "23         both  \n",
       "24         both  \n",
       "25         both  \n",
       "26         both  \n",
       "27         both  \n",
       "28         both  \n",
       "29         both  \n",
       "..          ...  \n",
       "73         both  \n",
       "74         both  \n",
       "75         both  \n",
       "76         both  \n",
       "77         both  \n",
       "78         both  \n",
       "79         both  \n",
       "80         both  \n",
       "81         both  \n",
       "82         both  \n",
       "83         both  \n",
       "84         both  \n",
       "85         both  \n",
       "86         both  \n",
       "87         both  \n",
       "88         both  \n",
       "89         both  \n",
       "90         both  \n",
       "91         both  \n",
       "92         both  \n",
       "93         both  \n",
       "94         both  \n",
       "95         both  \n",
       "96         both  \n",
       "97         both  \n",
       "98         both  \n",
       "99         both  \n",
       "100  right_only  \n",
       "101  right_only  \n",
       "102  right_only  \n",
       "\n",
       "[103 rows x 7 columns]"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impVocabBoth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Male professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Male professors\n",
    "maleSet_df = revPosNeg[(revPosNeg['department'].isin(['English','Mathematics','Biology','Psychology']))&(revPosNeg['genderBest']=='male')]\n",
    "maleSet_df = revSet_df[['profID','genderBest','department','sentiment','cleanText','words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only use reviews where text is not \"no comments\"\n",
    "maleSet_df = maleSet_df[revSet_df['cleanText']!=\"no comments \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick out the train and the test sets\n",
    "train_set = maleSet_df\n",
    "\n",
    "#pulling out the test set\n",
    "test_set = train_set.sample(frac=.2,random_state=2411)\n",
    "train_set=train_set.drop(test_set.index)\n",
    "# training to the sentiment\n",
    "y_train = train_set['sentiment']\n",
    "y_test = test_set['sentiment']\n",
    "\n",
    "\n",
    "# Initialize TFIDF vectorizor object (no n-grams)\n",
    "vectorizer = TfidfVectorizer( max_features = 40000, sublinear_tf = True )\n",
    "\n",
    "# fit_transform() fits the model & and learns the vocab (features)\n",
    "# then it transforms the training data into feature vectors. \n",
    "# The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(train_set['cleanText'])\n",
    "\n",
    "# convert to array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "# oob_score gives \"out of bag\" errors\n",
    "forest = RandomForestClassifier(n_estimators = 100,oob_score=True) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab = feature names\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92559838085181279"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how does the train set do on the out of bag reviews?\n",
    "forest.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(test_set['cleanText'])\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"profID\" & sentiment prediction\n",
    "output = pd.DataFrame( data={'sentimentActual':test_set[\"sentiment\"], 'gender':test_set['genderBest'],'depart':test_set['department'], \"sentimentPredict\":result} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maleImportances = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maleImp_df = pd.DataFrame({'vocab':vocab,'import':femImportances}).sort_values('import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imp_df = imp_df.reset_index()\n",
    "maleImp_df = maleImp_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>import</th>\n",
       "      <th>vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17910</th>\n",
       "      <td>17247</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>waste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17911</th>\n",
       "      <td>7678</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17912</th>\n",
       "      <td>5191</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17913</th>\n",
       "      <td>17431</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17914</th>\n",
       "      <td>1014</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17915</th>\n",
       "      <td>17519</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>willing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17916</th>\n",
       "      <td>6487</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17917</th>\n",
       "      <td>10678</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17918</th>\n",
       "      <td>435</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17919</th>\n",
       "      <td>6992</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17920</th>\n",
       "      <td>518</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>always</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17921</th>\n",
       "      <td>3126</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>confusing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17922</th>\n",
       "      <td>10369</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17923</th>\n",
       "      <td>2117</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17924</th>\n",
       "      <td>5309</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17925</th>\n",
       "      <td>6203</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17926</th>\n",
       "      <td>13637</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>rude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17927</th>\n",
       "      <td>12942</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17928</th>\n",
       "      <td>2608</td>\n",
       "      <td>0.005091</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17929</th>\n",
       "      <td>17873</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17930</th>\n",
       "      <td>4397</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>dont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17931</th>\n",
       "      <td>10742</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17932</th>\n",
       "      <td>6893</td>\n",
       "      <td>0.005542</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17933</th>\n",
       "      <td>16213</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17934</th>\n",
       "      <td>16621</td>\n",
       "      <td>0.005713</td>\n",
       "      <td>unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17935</th>\n",
       "      <td>15949</td>\n",
       "      <td>0.005846</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17936</th>\n",
       "      <td>4687</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17937</th>\n",
       "      <td>16043</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17938</th>\n",
       "      <td>17046</td>\n",
       "      <td>0.006253</td>\n",
       "      <td>very</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17939</th>\n",
       "      <td>10452</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17940</th>\n",
       "      <td>6177</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17941</th>\n",
       "      <td>15844</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>terrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17942</th>\n",
       "      <td>1820</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17943</th>\n",
       "      <td>7987</td>\n",
       "      <td>0.006742</td>\n",
       "      <td>interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17944</th>\n",
       "      <td>7074</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17945</th>\n",
       "      <td>15672</td>\n",
       "      <td>0.006905</td>\n",
       "      <td>teacher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17946</th>\n",
       "      <td>530</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17947</th>\n",
       "      <td>8147</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17948</th>\n",
       "      <td>1167</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>awful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17949</th>\n",
       "      <td>12047</td>\n",
       "      <td>0.007427</td>\n",
       "      <td>professor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17950</th>\n",
       "      <td>1124</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>avoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17951</th>\n",
       "      <td>7363</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17952</th>\n",
       "      <td>15667</td>\n",
       "      <td>0.009459</td>\n",
       "      <td>teach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17953</th>\n",
       "      <td>4355</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>doesnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17954</th>\n",
       "      <td>601</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17955</th>\n",
       "      <td>1150</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17956</th>\n",
       "      <td>1534</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17957</th>\n",
       "      <td>17718</td>\n",
       "      <td>0.020703</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17958</th>\n",
       "      <td>6628</td>\n",
       "      <td>0.027892</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17959</th>\n",
       "      <td>10526</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index    import        vocab\n",
       "17910  17247  0.003319        waste\n",
       "17911   7678  0.003396           in\n",
       "17912   5191  0.003411         ever\n",
       "17913  17431  0.003446         when\n",
       "17914   1014  0.003454           at\n",
       "17915  17519  0.003626      willing\n",
       "17916   6487  0.003768         good\n",
       "17917  10678  0.003884           of\n",
       "17918    435  0.003945          all\n",
       "17919   6992  0.003948           he\n",
       "17920    518  0.004350       always\n",
       "17921   3126  0.004419    confusing\n",
       "17922  10369  0.004423        never\n",
       "17923   2117  0.004529          but\n",
       "17924   5309  0.004695    excellent\n",
       "17925   6203  0.004718        funny\n",
       "17926  13637  0.004889         rude\n",
       "17927  12942  0.004939       really\n",
       "17928   2608  0.005091        class\n",
       "17929  17873  0.005161          you\n",
       "17930   4397  0.005516         dont\n",
       "17931  10742  0.005522           on\n",
       "17932   6893  0.005542         hard\n",
       "17933  16213  0.005653           to\n",
       "17934  16621  0.005713      unclear\n",
       "17935  15949  0.005846          the\n",
       "17936   4687  0.006025         easy\n",
       "17937  16043  0.006219         this\n",
       "17938  17046  0.006253         very\n",
       "17939  10452  0.006333           no\n",
       "17940   6177  0.006352          fun\n",
       "17941  15844  0.006404     terrible\n",
       "17942   1820  0.006724       boring\n",
       "17943   7987  0.006742  interesting\n",
       "17944   7074  0.006818      helpful\n",
       "17945  15672  0.006905      teacher\n",
       "17946    530  0.006946      amazing\n",
       "17947   8147  0.007144           is\n",
       "17948   1167  0.007228        awful\n",
       "17949  12047  0.007427    professor\n",
       "17950   1124  0.008455        avoid\n",
       "17951   7363  0.008827     horrible\n",
       "17952  15667  0.009459        teach\n",
       "17953   4355  0.009580       doesnt\n",
       "17954    601  0.010363          and\n",
       "17955   1150  0.010734      awesome\n",
       "17956   1534  0.018739         best\n",
       "17957  17718  0.020703        worst\n",
       "17958   6628  0.027892        great\n",
       "17959  10526  0.027931          not"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maleImp_df[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index_all</th>\n",
       "      <th>import_all</th>\n",
       "      <th>vocab</th>\n",
       "      <th>index_fem</th>\n",
       "      <th>import_fem</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17865.0</td>\n",
       "      <td>5201.0</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>even</td>\n",
       "      <td>5181.0</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17866.0</td>\n",
       "      <td>2273.0</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>cares</td>\n",
       "      <td>2273.0</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17867.0</td>\n",
       "      <td>15155.0</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>students</td>\n",
       "      <td>15141.0</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17868.0</td>\n",
       "      <td>10810.0</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>only</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17869.0</td>\n",
       "      <td>6777.0</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>guy</td>\n",
       "      <td>6755.0</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index_all  import_all     vocab  index_fem  import_fem     _merge\n",
       "0  17865.0     5201.0    0.002149      even     5181.0    0.002188       both\n",
       "1  17866.0     2273.0    0.002150     cares     2273.0    0.002369       both\n",
       "2  17867.0    15155.0    0.002157  students    15141.0    0.002213       both\n",
       "3  17868.0    10810.0    0.002163      only        NaN         NaN  left_only\n",
       "4  17869.0     6777.0    0.002203       guy     6755.0    0.002253       both"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impVocabBoth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impVocabMF=pd.merge(femImp_df[-100:],maleImp_df[-100:],on='vocab',how='outer',suffixes=('_fem','_male'),indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impVocabMF['diff']=impVocabMF['import_fem']-impVocabMF['import_male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impVocabMF = impVocabMF[['vocab','import_fem','import_male']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impVocabMF['femRank']=impVocabMF['import_fem'].rank(ascending=0)\n",
    "impVocabMF['maleRank']=impVocabMF['import_male'].rank(ascending=0)\n",
    "impVocabMF['rankDiff']=impVocabMF['femRank']-impVocabMF['maleRank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>import_fem</th>\n",
       "      <th>import_male</th>\n",
       "      <th>femRank</th>\n",
       "      <th>maleRank</th>\n",
       "      <th>rankDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>like</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>50.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>or</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>64.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dr</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>79.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>nothing</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>43.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yourself</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>103.5</td>\n",
       "      <td>-12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>makes</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prof</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.0</td>\n",
       "      <td>103.5</td>\n",
       "      <td>-10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>love</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>52.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>this</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>take</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>54.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>be</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>77.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>always</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>32.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>even</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>86.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sucks</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>103.5</td>\n",
       "      <td>-7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>easy</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>17.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>terrible</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>own</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>103.5</td>\n",
       "      <td>-6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>if</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>60.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>highly</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>61.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>with</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>68.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>her</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>48.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>math</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>103.5</td>\n",
       "      <td>-5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>really</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>him</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>83.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>guy</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>85.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>class</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>0.005091</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>about</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>103.5</td>\n",
       "      <td>-4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>so</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>72.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>that</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>81.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>will</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>70.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cares</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>87.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>unclear</td>\n",
       "      <td>0.005280</td>\n",
       "      <td>0.005713</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>but</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>teacher</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.006905</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>unhelpful</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>103.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>84.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>on</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>funny</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>39.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>rude</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>38.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>too</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>103.5</td>\n",
       "      <td>99.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>good</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>49.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>all</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>47.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>have</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>71.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lot</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>88.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>his</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>74.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>useless</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>103.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>helpful</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>just</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>89.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>had</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>65.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ever</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>57.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>do</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>73.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>amazing</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>24.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>willing</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>56.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>then</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>66.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>book</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>103.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>fun</td>\n",
       "      <td>0.004656</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>fair</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>103.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>waste</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>67.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>impossible</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>90.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>off</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>103.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          vocab  import_fem  import_male  femRank  maleRank  rankDiff\n",
       "50         like    0.003423     0.002654     50.0      69.0     -19.0\n",
       "36           or    0.002858     0.002441     64.0      79.0     -15.0\n",
       "21           dr    0.002512     0.002196     79.0      93.0     -14.0\n",
       "57      nothing    0.003832     0.003037     43.0      56.0     -13.0\n",
       "9      yourself    0.002242     0.000000     91.0     103.5     -12.5\n",
       "20        makes    0.002454     0.002245     80.0      91.0     -11.0\n",
       "7          prof    0.002176     0.000000     93.0     103.5     -10.5\n",
       "48         love    0.003103     0.002873     52.0      62.0     -10.0\n",
       "87         this    0.007387     0.006219     13.0      23.0     -10.0\n",
       "46         take    0.003061     0.002844     54.0      63.0      -9.0\n",
       "23           be    0.002573     0.002363     77.0      86.0      -9.0\n",
       "68       always    0.005144     0.004350     32.0      40.0      -8.0\n",
       "14         even    0.002349     0.002188     86.0      94.0      -8.0\n",
       "4         sucks    0.002137     0.000000     96.0     103.5      -7.5\n",
       "83         easy    0.006826     0.006025     17.0      24.0      -7.0\n",
       "88     terrible    0.007479     0.006404     12.0      19.0      -7.0\n",
       "3           own    0.002117     0.000000     97.0     103.5      -6.5\n",
       "40           if    0.002953     0.002796     60.0      66.0      -6.0\n",
       "39       highly    0.002897     0.002733     61.0      67.0      -6.0\n",
       "32         with    0.002754     0.002589     68.0      74.0      -6.0\n",
       "52          her    0.003430     0.003186     48.0      54.0      -6.0\n",
       "2          math    0.002094     0.000000     98.0     103.5      -5.5\n",
       "72       really    0.005412     0.004939     28.0      33.0      -5.0\n",
       "17          him    0.002388     0.002278     83.0      88.0      -5.0\n",
       "15          guy    0.002355     0.002253     85.0      90.0      -5.0\n",
       "73        class    0.005609     0.005091     27.0      32.0      -5.0\n",
       "1         about    0.002092     0.000000     99.0     103.5      -4.5\n",
       "28           so    0.002694     0.002556     72.0      76.0      -4.0\n",
       "19         that    0.002392     0.002366     81.0      85.0      -4.0\n",
       "30         will    0.002698     0.002590     70.0      73.0      -3.0\n",
       "..          ...         ...          ...      ...       ...       ...\n",
       "13        cares    0.002336     0.002369     87.0      84.0       3.0\n",
       "71      unclear    0.005280     0.005713     29.0      26.0       3.0\n",
       "60          but    0.004273     0.004529     40.0      37.0       3.0\n",
       "82      teacher    0.006805     0.006905     18.0      15.0       3.0\n",
       "100   unhelpful    0.000000     0.002078    103.5     100.0       3.5\n",
       "16    wonderful    0.002377     0.002432     84.0      80.0       4.0\n",
       "67           on    0.004986     0.005522     33.0      29.0       4.0\n",
       "61        funny    0.004344     0.004718     39.0      35.0       4.0\n",
       "62         rude    0.004455     0.004889     38.0      34.0       4.0\n",
       "101         too    0.000000     0.002107    103.5      99.0       4.5\n",
       "51         good    0.003423     0.003768     49.0      44.0       5.0\n",
       "53          all    0.003499     0.003945     47.0      42.0       5.0\n",
       "29         have    0.002696     0.002818     71.0      65.0       6.0\n",
       "12          lot    0.002283     0.002386     88.0      82.0       6.0\n",
       "26          his    0.002653     0.002721     74.0      68.0       6.0\n",
       "102     useless    0.000000     0.002115    103.5      97.0       6.5\n",
       "77      helpful    0.005756     0.006818     23.0      16.0       7.0\n",
       "11         just    0.002271     0.002426     89.0      81.0       8.0\n",
       "35          had    0.002830     0.002996     65.0      57.0       8.0\n",
       "43         ever    0.002979     0.003411     57.0      48.0       9.0\n",
       "27           do    0.002673     0.002824     73.0      64.0       9.0\n",
       "76      amazing    0.005728     0.006946     24.0      14.0      10.0\n",
       "44      willing    0.003040     0.003626     56.0      45.0      11.0\n",
       "34         then    0.002789     0.003200     66.0      53.0      13.0\n",
       "103        book    0.000000     0.002266    103.5      89.0      14.5\n",
       "65          fun    0.004656     0.006352     35.0      20.0      15.0\n",
       "104        fair    0.000000     0.002347    103.5      87.0      16.5\n",
       "33        waste    0.002768     0.003319     67.0      50.0      17.0\n",
       "10   impossible    0.002260     0.002626     90.0      72.0      18.0\n",
       "105         off    0.000000     0.002654    103.5      70.0      33.5\n",
       "\n",
       "[106 rows x 6 columns]"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impVocabMF.sort_values('rankDiff')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:my_projects_env]",
   "language": "python",
   "name": "conda-env-my_projects_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
